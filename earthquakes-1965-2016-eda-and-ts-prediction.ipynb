{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Author: Pierre Jeanne\n# Project Name: Earthquakes, 1965-2016\n# Date Created: 02 April 2021\n# from: https://www.kaggle.com/usgs/earthquake-database","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## INTRODUCTION\nThis dataset includes a record of the date, time, location, depth, magnitude, and source of every earthquake with a reported magnitude 5.5 or higher since 1965.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n# data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns; sns.set()\n# stat on data\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\nfrom statsmodels.tsa.arima.model import ARIMA\nimport cartopy.feature as cfeature","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(r\"../input/earthquake-database/database.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Shape of the file')\nprint('-'*30)\nprint(df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>1- Data Cleaning</center></h3>","metadata":{}},{"cell_type":"markdown","source":"## 1.1: Verify if NaN values:","metadata":{}},{"cell_type":"code","source":"def msv1(data, thresh=20, color='black', edgecolor='black', width=15, height=3):\n    \"\"\"\n    SOURCE: https://www.kaggle.com/amiiiney/price-prediction-regularization-stacking\n    \"\"\"\n    \n    plt.figure(figsize=(width,height))\n    percentage=(data.isnull().mean())*100\n    percentage.sort_values(ascending=False).plot.bar(color=color, edgecolor=edgecolor)\n    plt.axhline(y=thresh, color='r', linestyle='-')\n    plt.title('Missing values percentage per column', fontsize=20, weight='bold' )\n    plt.text(len(data.isnull().sum()/len(data))/1.7, thresh+12.5, f'Columns with more than {thresh}% missing values', fontsize=12, color='crimson',\n         ha='left' ,va='top')\n    plt.text(len(data.isnull().sum()/len(data))/1.7, thresh - 5, f'Columns with less than {thresh} missing values', fontsize=12, color='green',\n         ha='left' ,va='top')\n    plt.xlabel('Columns', size=15, weight='bold')\n    plt.ylabel('Missing values percentage')\n    \n    return plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msv1(df, 30, color=sns.color_palette('Reds',15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop all the columns with more than 30% of missing values\ndf = df.dropna(thresh=len(df)*0.70, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2: replace nan values","metadata":{}},{"cell_type":"code","source":"# create list with columns having NaN values\ncol_with_NaN_value = df.columns[df.isnull().any()]\n# create df with only columns having NaN value \ndf_nan = df[col_with_NaN_value]\n# count number of NaN value per columns\nprint('Number of NaN values')\nprint('-'*30)\ndf_nan.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop rows with the 3 missing values in 'magnitude type'\ndf = df.dropna(subset=['Magnitude Type'])\n\n# replace nan value in RMS with mean columns\ndf['Root Mean Square'] = df['Root Mean Square'].fillna(value=df['Root Mean Square'].mean())\n\n# verify operations:\ndf.isnull().sum()\n\n# reset index\ndf = df.reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3: Verify dtypes","metadata":{}},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.4: Parsing datetime","metadata":{}},{"cell_type":"code","source":"#exploring the length of date objects\nlengths = df[\"Date\"].str.len()\nlengths.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataframe contans 3 rows with wrong dates (24 characters). Let's have a look at these 3 dates.","metadata":{}},{"cell_type":"code","source":"#having a look at the fishy datapoints\nwrongdates = np.where([lengths == 24])[1]\nprint(\"Row index with wrong dates:\", wrongdates)\ndf.loc[wrongdates]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fixing the wrong dates \ndf.loc[3378, \"Date\"] = \"02/23/1975\"  \ndf.loc[7510, \"Date\"] = \"04/28/1985\"\ndf.loc[20647, \"Date\"] = \"03/13/2011\"\n\n#fixing the wrong Time \ndf.loc[3378, \"Time\"] = \"02:58:41\"  \ndf.loc[7510, \"Time\"] = \"02:53:41\"\ndf.loc[20647, \"Time\"] = \"02:23:34\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#exploring the length of time objects\nlengths = df[\"Time\"].str.len()\nlengths.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# datetime conversions merge Date + Time\ndf['datetime'] = df.Date+' '+df.Time\ndf['datetime'] = pd.to_datetime(df['datetime'], format='%m/%d/%Y %H:%M:%S')\n\n# drop irrelevant columns\ndf = df.drop(['Date', 'Time','index'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.5: Verify Value consistency","metadata":{}},{"cell_type":"code","source":"# Print unique values for categorical variables:\nprint('Type: ', df['Type'].unique(), \"\\n\")\nprint('Magnitude Type', df['Magnitude Type'].unique(), \"\\n\")\nprint('Source', df['Source'].unique(), \"\\n\")\nprint('Location Source', df['Location Source'].unique(), \"\\n\")          \nprint('Magnitude Source', df['Magnitude Source'].unique(), \"\\n\")\nprint('Status', df['Status'].unique(), \"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(15,10))\nfig.subplots_adjust(hspace=0.4,wspace=0.3)\n\n# subplot 1\nax1 = fig.add_subplot(2,2,1)\nax1=df['Type'].value_counts(normalize=True).plot.pie(legend=False,shadow=True, autopct='%1.1f%%' ,\n                                                              pctdistance=0.75, radius=1.05, wedgeprops = {'linewidth': 0.1}, \n                                                              textprops = {'fontsize': 12},normalize=False)\nax1.set_title('Type distribution', fontsize=16, weight='bold')\nax1.legend(fancybox=True, shadow=True, title='Type', fontsize=11,loc='upper right', bbox_to_anchor=(1.7, 1))\nax1.set_ylabel(\"\")\n\n# subplot 2\nax2 = fig.add_subplot(2,2,2)\nax2=df['Magnitude Type'].value_counts(normalize=True).plot.pie(legend=False,shadow=True, autopct='%1.1f%%' ,\n                                                              pctdistance=0.75, radius=1.05, wedgeprops = {'linewidth': 0.1}, \n                                                              textprops = {'fontsize': 12},normalize=False)\n\nax2.set_title('Magnitude Type distribution', fontsize=16, weight='bold')\nax2.legend(fancybox=True, shadow=True, title='Mg type', fontsize=11,loc='upper right', bbox_to_anchor=(1.7, 1))\nax2.set_ylabel(\"\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>2- Magnitude Conversion and  Earthquake locations</center></h3>","metadata":{}},{"cell_type":"code","source":"# keep only earthquake\ndf = df[df['Type']=='Earthquake']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Homogeneisation of the seismic magnitude\nSeismic magnitude scales (Ml, Ms, Md etc.) are used to describe the overall strength or \"size\" of an earthquake. Magnitudes are usually determined from measurements of an earthquake's seismic waves as recorded on a seismogram. Magnitude scales vary on what aspect of the seismic waves are measured and how they are measured. Different magnitude scales are necessary because of differences in earthquakes, the information available, and the purposes for which the magnitudes are used.\n\nMoment magnitude (Mw) is considered the authoritative magnitude scale for ranking earthquakes by size. It is more directly related to the energy of an earthquake than other scales, and does not saturate—that is, it does not underestimate magnitudes as other scales do in certain conditions. It has become the standard scale used by seismological authorities . Subtypes of the moment magnitude scale (Mww , etc.) reflect different ways of estimating the seismic moment.\n\nOver the year empirical relations where developped to link the Seismic magnitude scales  to Mw. These relations are not perfect and most of the time are site specific. However, we will try to convert all the seismic magnitude into the Moment magnitude.","metadata":{}},{"cell_type":"code","source":"### ----------  Equations present in the literacy to convert Seismic magnitude to Moment magnitude--------------\n#--------- Thatcher & Hanks (1973) \n# Mo = 10^(Ml *1.5 + 9.0)   \n#--------- Kanamori 1977.\n# Mw = (2/3)*log10(10^(Mo)) - 10.7\n#--------- Sitaram and Bora (2007) \n# Md = 1.5*Ml -0.17\n#--------- (Scordilis, 2006):\n# -- for 3.0 ≤ MS ≤ 6.1,\n# Mw = 0.67 MS + 2.07\n# -- for 6.2 ≤ MS ≤ 8.2\n# Mw = 0.99MS + 0.08\n# -- for 3.5 ≤ mb ≤ 6.2\n# Mw = 0.85mb + 1.03\n\ncondlist = [df['Magnitude Type']== 'ML',\n            df['Magnitude Type']== 'MD',\n           (df['Magnitude Type']== 'MS')&(df['Magnitude']<= 6.1),\n           (df['Magnitude Type']== 'MS')&(df['Magnitude']> 6.1),\n           (df['Magnitude Type']== 'MB')&(df['Magnitude']<= 6.2)]\n\nchoicelist = [(2/3)*np.log10(10**((df['Magnitude'] *1.5) + 9.0))-10.7,\n              0.93*df['Magnitude'] + 0.35,\n              0.67*df['Magnitude'] + 2.07,\n              0.99*df['Magnitude'] + 0.08,\n              0.85*df['Magnitude'] + 1.03]\ndf['Moment magnitude'] =  np.select(condlist, choicelist,default = df['Magnitude'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### comparasison magnitude vs seismic moment","metadata":{}},{"cell_type":"code","source":"import cartopy.crs as ccrs\nimport cartopy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\n\n# plot map with magnitude\nax1 = plt.axes(projection=ccrs.PlateCarree())\nsc = ax1.scatter(df['Longitude'], df['Latitude'],s= df['Magnitude'],c=df['Magnitude'], transform=ccrs.PlateCarree(),\n           label=df['Magnitude'])\n# produce a legend with the unique colors from the scatter\nlegend1 = ax1.legend(*sc.legend_elements(),loc=\"upper right\", title=\"magnitudes\",bbox_to_anchor=[1.2,1])\nax1.add_artist(legend1)\n\nax1.stock_img()\nax1.coastlines()\n\nax1.set_global()\nax1.gridlines(draw_labels=True, dms=False, x_inline=False, y_inline=False)\n\nax1.set_title('Earthquake locations 1965-2016')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\n# plot map with magnitude\nax2 = plt.axes(projection=ccrs.PlateCarree())\nsc = ax2.scatter(df['Longitude'], df['Latitude'],s= df['Moment magnitude']*2,c=df['Moment magnitude'], transform=ccrs.PlateCarree(),\n           label=df['Moment magnitude'])\n# produce a legend with the unique colors from the scatter\nlegend2 = ax2.legend(*sc.legend_elements(),loc=\"upper right\", title=\"Moment magnitude\",bbox_to_anchor=[1.2,1])\nax2.add_artist(legend2)\n\nax2.stock_img()\nax2.coastlines()\n\nax2.set_global()\nax2.gridlines(draw_labels=True, dms=False, x_inline=False, y_inline=False)\n\nax2.set_title('Earthquake locations 1965-2016')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some of the calculated Moment magnitudes are way too low. It doesn't work... I will keep working with the different earthquake magnitude","metadata":{}},{"cell_type":"code","source":"df =df.drop(['Type','ID', 'Source', 'Location Source',\n              'Magnitude Source', 'Status','Moment magnitude'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>3- Earthquake clustering and feature engineering</center></h3>","metadata":{}},{"cell_type":"markdown","source":"The goal here is to classify the eartquake into 4 groups:\n\n- **Extension**: ridge oceanique \n- **Subduction**: the oceanic lithosphere dives beneath the continental lithosphere\n- **Convergence**: two continental lithospheres collide\n- **Transform**: tectonic plates slide sideways past each other\n\nThe strategy is to create many small clusters (25), looked at their locations, assign them to a tectonic setting, and do some correction manually if needed.","metadata":{}},{"cell_type":"code","source":"## ------------------- 1 - Create 25 clusters  -----------------\n# Import KMeans\nfrom sklearn.cluster import KMeans\n\nX = df[['Latitude','Longitude']].values\n\n# Create a KMeans instance with 3 clusters: model\nmodel = KMeans(n_clusters=25, random_state=2)\n\n# Fit model to points\nmodel.fit(X)\ncluster = model.predict(X)\n\n# add results to df \ndf['cluster'] = 'None'\ndf['cluster'] = pd.Series(cluster, index=df.index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## ------------------- 2- plot the 25 cluster on a map  -----------------\nplt.figure(figsize=(10,10))\n\nax = plt.axes(projection=ccrs.PlateCarree())\nsc = ax.scatter(df['Longitude'], df['Latitude'],c=df['cluster'], transform=ccrs.PlateCarree(),\n           label=df['cluster'])\nax.stock_img()\nax.coastlines()\nax.gridlines(draw_labels=True, dms=False, x_inline=False, y_inline=False)\nax.set_title('Earthquake clustering')\n\nax.set_global()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## -------------------  3- assign each cluster to the relevant geological setting\ndf.loc[(df['cluster']== 0), ['Tecto-setting',\"region\"]] = 'Subduction','Pacific plate West'\ndf.loc[(df['cluster']== 1), ['Tecto-setting',\"region\"]] = 'Subduction','Philipine sea plate'\ndf.loc[(df['cluster']== 2), ['Tecto-setting',\"region\"]] = 'Subduction','Carribbean plate'\ndf.loc[(df['cluster']== 3), ['Tecto-setting',\"region\"]] = 'Convergence','Europe'\ndf.loc[(df['cluster']== 4), ['Tecto-setting',\"region\"]] = 'Subduction','Pacific plate West'\ndf.loc[(df['cluster']== 5), ['Tecto-setting',\"region\"]] = 'Subduction','Pacific plate North'\ndf.loc[(df['cluster']== 6), ['Tecto-setting',\"region\"]] = 'Extension','Antartic plate'\ndf.loc[(df['cluster']== 7), ['Tecto-setting',\"region\"]] = 'Convergence','Indian plate'\ndf.loc[(df['cluster']== 8), ['Tecto-setting',\"region\"]] = 'Subduction','Pacific plate North'\ndf.loc[(df['cluster']== 9), ['Tecto-setting',\"region\"]] = 'Extension','Antartic plate'\ndf.loc[(df['cluster']== 10), ['Tecto-setting',\"region\"]] = 'Subduction','Philipine sea plate'\ndf.loc[(df['cluster']== 11), ['Tecto-setting',\"region\"]] = 'Subduction','Nazca plate'\ndf.loc[(df['cluster']== 12), ['Tecto-setting',\"region\"]] = 'Extension','Antartic plate'\ndf.loc[(df['cluster']== 13), ['Tecto-setting',\"region\"]] = 'Transform','Pacific plate East'\ndf.loc[(df['cluster']== 14), ['Tecto-setting',\"region\"]] = 'Extension','Antartic plate'\ndf.loc[(df['cluster']== 15), ['Tecto-setting',\"region\"]] = 'Subduction','Pacific plate West'\ndf.loc[(df['cluster']== 16), ['Tecto-setting',\"region\"]] = 'Extension','Ridge Atlantic'\ndf.loc[(df['cluster']== 17), ['Tecto-setting',\"region\"]] = 'Extension','Antartic plate'\ndf.loc[(df['cluster']== 18), ['Tecto-setting',\"region\"]] = 'Subduction','Australian plate'\ndf.loc[(df['cluster']== 19), ['Tecto-setting',\"region\"]] = 'Extension','Pacific plate East'\ndf.loc[(df['cluster']== 20), ['Tecto-setting',\"region\"]] = 'Subduction','Philipine sea plate'\ndf.loc[(df['cluster']== 21), ['Tecto-setting',\"region\"]] = 'Convergence','Western Asia'\ndf.loc[(df['cluster']== 22), ['Tecto-setting',\"region\"]] = 'Extension','Ridge Atlantic'\ndf.loc[(df['cluster']== 23), ['Tecto-setting',\"region\"]] = 'Subduction','Cosco plate'\ndf.loc[(df['cluster']== 24), ['Tecto-setting',\"region\"]] = 'Subduction','Philipine sea plate'\n\n# drop cluster\ndf =df.drop(['cluster'], axis=1)\n\n## ---------------- 4- fix manually few mistakes \ndf.loc[(df['Longitude']>=0)&(df['Longitude']<150)&\n       (df['Latitude']>=70)&(df['Latitude']<=90), ['Tecto-setting',\"region\"]] = 'Extension','Rigde Artic'\n     \ndf.loc[(df['Longitude']>=0)&(df['Longitude']<43)\n             &(df['Latitude']>=-5)&(df['Latitude']<=30), ['Tecto-setting','region']] = 'Extension','Arabian plate'\n       \ndf.loc[(df['Longitude']>=43)&(df['Longitude']<70)\n             &(df['Latitude']>=-5)&(df['Latitude']<=19), ['Tecto-setting','region']] = 'Extension','Arabian plate'\n\ndf.loc[(df['Longitude']>=-120)&(df['Longitude']<-100)\n             &(df['Latitude']>=-5)&(df['Latitude']<=13), ['Tecto-setting','region']] = 'Extension','Pacific plate East'\n\ndf.loc[(df['Longitude']>=-100)&(df['Longitude']<-90)\n             &(df['Latitude']>=-5)&(df['Latitude']<=5), ['Tecto-setting','region']] = 'Extension','Pacific plate East'\n\ndf.loc[(df['Longitude']>=-90)&(df['Longitude']<-80)\n             &(df['Latitude']>=-70)&(df['Latitude']<=-30), ['Tecto-setting','region']] = 'Extension','Antartic plate'\n\ndf.loc[(df['Longitude']>=-100)&(df['Longitude']<-68)\n             &(df['Latitude']>=-15)&(df['Latitude']<=12), ['Tecto-setting','region']] = 'Subduction','Nazca plate'\n\ndf.loc[(df['Longitude']>=-20)&(df['Longitude']<-0)\n             &(df['Latitude']>=-53)&(df['Latitude']<=23), ['Tecto-setting','region']] = 'Extension','Ridge Atlantic'\n\ndf.loc[(df['Longitude']>=156)&(df['Longitude']<180)\n             &(df['Latitude']>=-56)&(df['Latitude']<=38), ['Tecto-setting','region']] = 'Subduction','Pacific plate West'\n\ndf.loc[(df['Longitude']>=20)&(df['Longitude']<50)\n             &(df['Latitude']>=-32)&(df['Latitude']<=0), ['Tecto-setting','region']] = 'Extension','Africain rift'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create map with the different geological setting\ndf_convergence = df[df['Tecto-setting'] == 'Convergence']\ndf_subduction = df[df['Tecto-setting'] == 'Subduction']\ndf_extension = df[df['Tecto-setting'] == 'Extension']\ndf_transform = df[df['Tecto-setting'] == 'Transform']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create map with the different geological setting\nplt.figure(figsize=(10,10))\n# plot map\nax = plt.axes(projection=ccrs.PlateCarree())\nsc = sns.scatterplot(x= 'Longitude', y='Latitude', transform=ccrs.PlateCarree(),data=df, hue='Tecto-setting')\n\nax.legend(loc=\"upper right\", title=\" Tectonic setting\",bbox_to_anchor=[1.3,1])\nax.stock_img()\nax.coastlines()\nax.gridlines(draw_labels=True, dms=False, x_inline=False, y_inline=False)\n\nax.set_global()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create map with the different geological setting\nplt.figure(figsize=(10,10))\n# plot map\nax = plt.axes(projection=ccrs.PlateCarree())\nsc = sns.scatterplot(x= 'Longitude', y='Latitude', transform=ccrs.PlateCarree(),data=df, hue='region')\n\nax.legend(loc=\"upper right\", title=\" Regions\",bbox_to_anchor=[1.3,1])\nax.stock_img()\nax.coastlines()\nax.gridlines(draw_labels=True, dms=False, x_inline=False, y_inline=False)\n\nax.set_global()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>4- EDA</center></h3>","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(5,5))\n\n# subplot 1\nax = df['Tecto-setting'].value_counts(normalize=True).plot.pie(legend=False,shadow=True, autopct='%1.1f%%' ,\n                                                              pctdistance=0.75, radius=1.05, wedgeprops = {'linewidth': 0.1}, \n                                                              textprops = {'fontsize': 12},normalize=False)\nax.set_title('Earthquake vs tectonic setting', fontsize=16, weight='bold')\nax.legend(fancybox=True, shadow=True, title='Tectonic setting', fontsize=11,loc='upper right', bbox_to_anchor=(1.7, 1))\nax.set_ylabel(\"\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(15,4))\n\nax1 = fig.add_subplot(1,3,1)\nax1 = sns.boxplot(x=\"Tecto-setting\", y=\"Magnitude\", data=df)\n\nax2 = fig.add_subplot(1,3,2)\nax2 = sns.boxplot(x=\"Tecto-setting\", y=\"Root Mean Square\", data=df)\n\nax3 = fig.add_subplot(1,3,3)\nax3 = sns.boxplot(x=\"Tecto-setting\", y=\"Depth\", data=df)\n\n\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ecdf(data):\n    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n    # Number of data points: n\n    n = len(data)\n    # x-data for the ECDF: x \n    x = np.sort(data)\n    # y-data for the ECDF: y  The y data of the ECDF go from 1/n to 1 in equally spaced increments. \n    y = np.arange(1,n+1) / n\n    \n    return x, y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define the variable mags\nmags_convergence = df_convergence['Magnitude']\nmags_subduction = df_subduction['Magnitude']\nmags_extension = df_extension['Magnitude']\nmags_transform = df_transform['Magnitude']\n\n# define figure size\nfig = plt.figure(figsize=(10,8))\nfig.subplots_adjust(hspace=0.4,wspace=0.3)\n\n# figure title\nfig.suptitle('Empirical Cumulative Distribution Function', fontsize=18)\n\n# subplot 1\nax1 = fig.add_subplot(2,2,1)\nax1 = plt.plot(*ecdf(mags_extension),marker='.',linestyle = 'none')\nax1 = plt.xlabel('magnitude')\nax1 = plt.ylabel('ECDF')\nax1 = plt.text(6.5, 0.2, 'max magnitude {}'.format(mags_extension.max()),fontsize=12)\nax1 = plt.title('Extension regime')\n\n# subplot 2\nax2 = fig.add_subplot(2,2,2)\nax2 = plt.plot(*ecdf(mags_subduction),marker='.',linestyle = 'none',color='orange')\nax2 = plt.xlabel('magnitude')\nax2 = plt.ylabel('ECDF') \nax2 = plt.text(6.5, 0.2, 'max magnitude {}'.format(mags_subduction.max()),fontsize=12)\nax2 = plt.title('Subduction regime')\n\n# subplot 3\nax3 = fig.add_subplot(2,2,3)\nax3 = plt.plot(*ecdf(mags_convergence),marker='.',linestyle = 'none',color='green')\nax3 = plt.xlabel('magnitude')\nax3 = plt.ylabel('ECDF') \nax3 = plt.text(6.5, 0.2, 'max magnitude {}'.format(mags_convergence.max()),fontsize=12)\nax3 = plt.title('Convergence zone')\n\n# subplot 4\nax4 = fig.add_subplot(2,2,4)\nax4 = plt.plot(*ecdf(mags_transform),marker='.',linestyle = 'none',color='red')\nax4 = plt.xlabel('magnitude')\nax4 = plt.ylabel('ECDF')\nax4 = plt.text(6.5, 0.2, 'max magnitude {}'.format(mags_transform.max()),fontsize=12)\nax4 = plt.title('Transform zone')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2- earthquake properites vs region","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(5,5))\n\n# subplot 1\nax = df['region'].value_counts(normalize=True).plot.pie(legend=False,shadow=True, autopct='%1.1f%%' ,\n                                                              pctdistance=0.75, radius=1.05, wedgeprops = {'linewidth': 0.1}, \n                                                              textprops = {'fontsize': 12},normalize=False)\nax.set_title('Earthquake per region', fontsize=16, weight='bold')\nax.legend(fancybox=True, shadow=True, title='Region', fontsize=11,loc='upper right', bbox_to_anchor=(1.9, 1))\nax.set_ylabel(\"\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(15,4))\n\nax1 = fig.add_subplot(1,3,1)\ngrouped_mag = df.loc[:,['region', 'Magnitude']].groupby(['region']).median().sort_values(by='Magnitude')\nax1 = sns.boxplot(x=\"region\", y=\"Magnitude\", data=df,order = grouped_mag.index)\nax1 = plt.xticks(rotation=90)\n\nax2 = fig.add_subplot(1,3,2)\ngrouped_RMS = df.loc[:,['region', 'Root Mean Square']].groupby(['region']).median().sort_values(by='Root Mean Square')\nax2 = sns.boxplot(x=\"region\", y=\"Root Mean Square\", data=df,order = grouped_RMS.index)\nax2 = plt.xticks(rotation=90)\n\nax3 = fig.add_subplot(1,3,3)\ngrouped_depth = df.loc[:,['region', 'Depth']].groupby(['region']).median().sort_values(by='Depth')\nax3 = sns.boxplot(x=\"region\", y=\"Depth\", data=df,order = grouped_depth.index)\nax3 = plt.xticks(rotation=90)\n\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3- Time and distance between two consecutive earthquakes per region ","metadata":{}},{"cell_type":"code","source":"def calcul_distance_and_time_between_quake_per_region(region):\n    df_region = df[df['region']==region]\n    # need to calculate abs() to avoid pb with region having earthquake with both positive and negative lat or long\n    lat=abs(df_region['Latitude'])\n    long = abs(df_region['Longitude'])\n    # calucl distance two consecutive eartquakes: 1degree = 111 km\n    df_region['distance']= (((lat.diff())*111)**2 + ((long.diff())*111)**2 + \n                            (df_region['Depth'].diff())**2)**0.5\n    # calucl time between two consecutive eartquake:\n    df_region['delta'] = df_region['datetime'].diff().dt.days\n    # remove firt row\n    df_region = df_region.iloc[1:]\n    return df_region\n\ndf_Philipine = calcul_distance_and_time_between_quake_per_region('Philipine sea plate')\ndf_Pacific_west = calcul_distance_and_time_between_quake_per_region('Pacific plate West')\ndf_Antartic = calcul_distance_and_time_between_quake_per_region('Antartic plate')\ndf_Indian = calcul_distance_and_time_between_quake_per_region('Indian plate')\ndf_Australian = calcul_distance_and_time_between_quake_per_region('Australian plate')\ndf_Pacific_north = calcul_distance_and_time_between_quake_per_region('Pacific plate North')\ndf_west_asia = calcul_distance_and_time_between_quake_per_region('Western Asia')\ndf_Ridge_Atlantic = calcul_distance_and_time_between_quake_per_region('Ridge Atlantic')\ndf_Nazca = calcul_distance_and_time_between_quake_per_region('Nazca plate')\ndf_Cosco = calcul_distance_and_time_between_quake_per_region('Cosco plate')\ndf_Europe = calcul_distance_and_time_between_quake_per_region('Europe')\ndf_Pacific_east = calcul_distance_and_time_between_quake_per_region('Pacific plate East')\ndf_Arabian = calcul_distance_and_time_between_quake_per_region('Arabian plate')\ndf_Carribbean = calcul_distance_and_time_between_quake_per_region('Carribbean plate')\ndf_Africain_rift = calcul_distance_and_time_between_quake_per_region('Africain rift')\ndf_Rigde_Artic = calcul_distance_and_time_between_quake_per_region('Rigde Artic')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SeabornFig2grid: https://stackoverflow.com/questions/35042255/how-to-plot-multiple-seaborn-jointplot-in-subplot/47664533#47664533\n\n\nclass SeabornFig2Grid():\n\n    def __init__(self, seaborngrid, fig,  subplot_spec):\n        self.fig = fig\n        self.sg = seaborngrid\n        self.subplot = subplot_spec\n        if isinstance(self.sg, sns.axisgrid.FacetGrid) or \\\n            isinstance(self.sg, sns.axisgrid.PairGrid):\n            self._movegrid()\n        elif isinstance(self.sg, sns.axisgrid.JointGrid):\n            self._movejointgrid()\n        self._finalize()\n\n    def _movegrid(self):\n        \"\"\" Move PairGrid or Facetgrid \"\"\"\n        self._resize()\n        n = self.sg.axes.shape[0]\n        m = self.sg.axes.shape[1]\n        self.subgrid = gridspec.GridSpecFromSubplotSpec(n,m, subplot_spec=self.subplot)\n        for i in range(n):\n            for j in range(m):\n                self._moveaxes(self.sg.axes[i,j], self.subgrid[i,j])\n\n    def _movejointgrid(self):\n        \"\"\" Move Jointgrid \"\"\"\n        h= self.sg.ax_joint.get_position().height\n        h2= self.sg.ax_marg_x.get_position().height\n        r = int(np.round(h/h2))\n        self._resize()\n        self.subgrid = gridspec.GridSpecFromSubplotSpec(r+1,r+1, subplot_spec=self.subplot)\n\n        self._moveaxes(self.sg.ax_joint, self.subgrid[1:, :-1])\n        self._moveaxes(self.sg.ax_marg_x, self.subgrid[0, :-1])\n        self._moveaxes(self.sg.ax_marg_y, self.subgrid[1:, -1])\n\n    def _moveaxes(self, ax, gs):\n        #https://stackoverflow.com/a/46906599/4124317\n        ax.remove()\n        ax.figure=self.fig\n        self.fig.axes.append(ax)\n        self.fig.add_axes(ax)\n        ax._subplotspec = gs\n        ax.set_position(gs.get_position(self.fig))\n        ax.set_subplotspec(gs)\n\n    def _finalize(self):\n        plt.close(self.sg.fig)\n        self.fig.canvas.mpl_connect(\"resize_event\", self._resize)\n        self.fig.canvas.draw()\n\n    def _resize(self, evt=None):\n        self.sg.fig.set_size_inches(self.fig.get_size_inches())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_graph_Time_vs_distance(df,title):\n    #  subplot 1\n    g0 = sns.JointGrid(data=df, x=\"delta\", y=\"Magnitude\", ratio= 3, marginal_ticks=True)\n    g0.plot_joint(sns.scatterplot, s=50, alpha=.7)\n    g0.plot_joint(sns.kdeplot, color=\"r\",fill=True,alpha=.8)\n    g0.plot_joint(sns.kdeplot, color=\"k\",fill=False)\n    g0.plot_marginals(sns.histplot, kde=True)\n    # label\n    g0.set_axis_labels('Time between 2 consecutive earthquake', 'Magnitude', fontsize=16)\n\n    g1 = sns.JointGrid(data=df, x=\"delta\", y=\"distance\", ratio= 3, marginal_ticks=True)\n    g1.plot_joint(sns.scatterplot, s=50, alpha=.7,color='k')\n    g1.plot_joint(sns.kdeplot, color=\"g\",fill=True,alpha=.8)\n    g1.plot_joint(sns.kdeplot, color=\"k\",fill=False)\n    g1.plot_marginals(sns.histplot, kde=True)\n    # label\n    g1.set_axis_labels('Time between 2 consecutive earthquake', 'Distance', fontsize=16)\n\n    fig = plt.figure(figsize=(15,5))\n    gs = gridspec.GridSpec(1, 2)\n\n    mg0 = SeabornFig2Grid(g0, fig, gs[0])\n    mg1 = SeabornFig2Grid(g1, fig, gs[1])\n\n    gs.tight_layout(fig)\n\n    fig.suptitle('{}:'.format(title),fontsize=18,y=1.1)\n    \n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_graph_Time_vs_distance(df_Philipine,'Philipine sea plate')\ncreate_graph_Time_vs_distance(df_Pacific_west,'Pacific plate West')\ncreate_graph_Time_vs_distance(df_Antartic,'Antartic plate')\ncreate_graph_Time_vs_distance(df_Indian,'Indian plate')\ncreate_graph_Time_vs_distance(df_Australian,'Australian plate')\ncreate_graph_Time_vs_distance(df_Pacific_north,'Pacific plate North')\ncreate_graph_Time_vs_distance(df_west_asia,'Western Asia')\ncreate_graph_Time_vs_distance(df_Ridge_Atlantic,'Ridge Atlantic')\ncreate_graph_Time_vs_distance(df_Nazca,'Nazca plate')\ncreate_graph_Time_vs_distance(df_Cosco,'Cosco plate')\ncreate_graph_Time_vs_distance(df_Europe,'Europe')\ncreate_graph_Time_vs_distance(df_Pacific_east,'Pacific plate East')\ncreate_graph_Time_vs_distance(df_Arabian,'Arabian plate')\ncreate_graph_Time_vs_distance(df_Carribbean,'Carribbean plate')\ncreate_graph_Time_vs_distance(df_Africain_rift,'Africain rift')\ncreate_graph_Time_vs_distance(df_Rigde_Artic,'Rigde Artic')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The graphs on the left show that :\n- In general, earthquakes are followed by another earthquakes in the following 1 or 2 months, and this is especially true if the first earthake is of large magnitude. \n\nThe graphs on the right show :\n- In general, the second earthquake occurs shortly after and close to the first earthquake. However, the second earthquake may also happen everywhere along the plate boundarys, and so be located far from the first earthquake, but if the time between the two earthquakes increases the second earthquake will be closer to the last one. (see Indian plate, Nazca plate, Cosco plate, Europe)\n","metadata":{}},{"cell_type":"markdown","source":"## <a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>5: Machine learning: the Nazca plate</center></h3>","metadata":{}},{"cell_type":"markdown","source":"## 5.1: data preparation\nWe will try to predict the seismic energy released every month along the Nazca boundary plate and the monthly number of seismic events. \n\nThe seismic energy released during each seismic event is calculated from the moment magnitude (Mw), so we can drop the other type of magnitude.\n\n### 5.1.1: feature engineering\n","metadata":{}},{"cell_type":"code","source":"df_Nazca_Mw = df_Nazca.loc[(df_Nazca['Magnitude Type']!='MS')&(df_Nazca['Magnitude Type']!='MB')]\n\nprint('Number of seismic events dropped: {}'.format((df_Nazca.shape[0]-df_Nazca_Mw.shape[0])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # calcul the seismic moment = energy release during an earthquake\ndf_Nazca_Mw.loc[:,'Energy'] = 10**((3/2)*df_Nazca_Mw.loc[:,'Magnitude']+16.1)\n# add 'count' to count number of earthquake during a month \ndf_Nazca_Mw.loc[:,'count'] =1\n\ndf_Nazca_Mw = df_Nazca_Mw.set_index('datetime')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.1.2: create a time Series with a constant time intervals (here month).","metadata":{}},{"cell_type":"code","source":"df_Nazca_Mw = df_Nazca_Mw[['Energy','count']]\ndf_Nazca_Mw = df_Nazca_Mw.resample('M').sum()\ndf_Nazca_Mw['log_Energy'] = np.log(df_Nazca_Mw['Energy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy import inf\ndf_Nazca_Mw['log_Energy'][df_Nazca_Mw['log_Energy'] == -inf] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(2,1, figsize=(12, 10), sharex=True)\nax1, ax2 = axes.flatten()\n\nplt.suptitle('Monthly evolutions of seismic energy released  and number of earthquake along the Nazca plate',size=18) \n \n\nax1.plot(df_Nazca_Mw.index, df_Nazca_Mw[\"log_Energy\"],color='darkblue')\n# ax1.set_yscale('log')\nax1.set_ylabel('Monthly amount of seismic energy released', color='darkblue',fontsize=15)\nax1.tick_params(axis='both', which='major', labelsize=14)\nax1.tick_params('y', colors='darkblue')\n\nax2.plot(df_Nazca_Mw.index,df_Nazca_Mw['count'],color='green')\nax2.set_ylabel('Number of earthquake per month', color='green',fontsize=15)\nax2.tick_params('y', colors='green')\nax2.tick_params(axis='both', which='major', labelsize=14)\n\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see there are not data from ~1972 to ~1984, so I won't use the data befor ~1984.","metadata":{}},{"cell_type":"code","source":"df_Nazca_Mw = df_Nazca_Mw[df_Nazca_Mw.index >= '1982-08-01']\n\nfig, axes = plt.subplots(2,1, figsize=(12, 10), sharex=True)\nax1, ax2 = axes.flatten()\n\nplt.suptitle('Monthly evolutions of seismic energy released  and number of earthquake along the Nazca plate',size=18) \n\nax1.plot(df_Nazca_Mw.index, df_Nazca_Mw[\"log_Energy\"],color='darkblue')\n# ax1.set_yscale('log')\nax1.set_ylabel('Monthly amount of seismic energy released', color='darkblue',fontsize=15)\nax1.tick_params(axis='both', which='major', labelsize=14)\nax1.tick_params('y', colors='darkblue')\n\nax2.plot(df_Nazca_Mw.index,df_Nazca_Mw['count'],color='green')\nax2.set_ylabel('Number of earthquake per month', color='green',fontsize=15)\nax2.tick_params('y', colors='green')\nax2.tick_params(axis='both', which='major', labelsize=14)\n\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2: Check the  Stationarity of a Time Series\nWe can assume the series to be stationary if it has constant statistical properties over time, ie. the following:\n- constant mean\n- constant variance\n- an autocovariance that does not depend on time.\n\n### 5.2.1: Check the  Stationarity by Plotting Rolling Statistics: \nWe can plot the moving average and moving variance to see if they varies with time. ","metadata":{}},{"cell_type":"code","source":"# Determing rolling statistics for monthly energy released\nrolmean_energy = df_Nazca_Mw['log_Energy'].rolling(window=12).mean()\nrolstd_energy = df_Nazca_Mw['log_Energy'].rolling(window=12).std()\n\n# Determing rolling statistics for monthly number of earthquake\nrolmean_number = df_Nazca_Mw['count'].rolling(window=12).mean()\nrolstd_number = df_Nazca_Mw['count'].rolling(window=12).std()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot rolling statistics:\nfig, axes = plt.subplots(1,2, figsize=(15,6))\nax1, ax2 = axes.flatten()\n\nplt.suptitle('Rolling Mean & Standard Deviation',size=18) \n\nax1.plot(df_Nazca_Mw['log_Energy'],'b-',label = 'TS: energy')\nax1.plot(rolmean_energy,'r-',label = 'Rolling Mean')\nax1.plot(rolstd_energy,'k-',label = 'Rolling std')\nax1.set_ylabel('monthly seismic energy released', color='darkblue',fontsize=15)\nax1.tick_params(axis='both', which='major', labelsize=14)\nax1.tick_params('y', colors='darkblue')\nax1.legend(loc='best')\n\nax2.plot(df_Nazca_Mw['count'],'g-',label = 'TS: number quake')\nax2.plot(rolmean_number,'r-',label = 'Rolling Mean')\nax2.plot(rolstd_number,'k-',label = 'Rolling std')\nax2.set_ylabel('monthly number earthquake', color='green',fontsize=15)\nax2.tick_params(axis='both', which='major',color='green', labelsize=14)\nax2.tick_params('y', colors='darkblue')\nax2.legend(loc='best')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# other test:\nX = df_Nazca_Mw['log_Energy']\nsplit = len(X) // 2\nX1, X2 = X[0:split], X[split:]\nmean1, mean2 = X1.mean(), X2.mean()\nvar1, var2 = X1.var(), X2.var()\nprint('mean1=%f, mean2=%f' % (mean1, mean2))\nprint('variance1=%f, variance2=%f' % (var1, var2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observed that **the means** and **the standard deviations** seem to be more or less **constant** so these two time series are  **stationary**.\n\nHowever, to be sure we can perform The Dickey-Fuller test.","metadata":{}},{"cell_type":"markdown","source":"### 5.2.2: Check the  Stationarity with the Dickey-Fuller test:\nHere the null hypothesis is that the Time Series is non-stationary. The test results comprise of a Test Statistic and some Critical Values for difference confidence levels. If the ‘Test Statistic’ is less than the ‘Critical Value’, we can reject the null hypothesis and say that the series is stationary.","metadata":{}},{"cell_type":"code","source":"# Perform The Dickey-Fuller test\nfrom statsmodels.tsa.stattools import adfuller\n\nprint('result of The Dickey-Fuller test:')\ndftest = adfuller(df_Nazca_Mw['log_Energy'],autolag = 'AIC')\n\ndfoutput = pd.Series(dftest[0:4], index=['test Statistic','p-value','#Lags Used','Number of Observations Used'])\nfor key, value in dftest[4].items():\n    dfoutput['Critical Value (%s)'%key] = value\n    \nprint(dfoutput)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here:\n- the p-value is less than the significance level (0.05) then we can reject the null hypothesis and infer that the time series is indeed stationary.\n\n- Moreover, the test statistic (~-19) is less than the Critical Values (from -2 to -3) so indeed the series is stationary.","metadata":{}},{"cell_type":"code","source":"# Perform The Dickey-Fuller test\n\nprint('result of The Dickey-Fuller test:')\ndftest = adfuller(df_Nazca_Mw['count'],autolag = 'AIC')\n\ndfoutput = pd.Series(dftest[0:4], index=['test Statistic','p-value','#Lags Used','Number of Observations Used'])\nfor key, value in dftest[4].items():\n    dfoutput['Critical Value (%s)'%key] = value\n    \nprint(dfoutput)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here:\n- the p-value is less than the significance level (0.05) then we can reject the null hypothesis and infer that the time series is indeed stationary.\n\n- Moreover, the test statistic (~-15) is less than the Critical Values (from -2 to -3) so indeed the series is stationary.","metadata":{}},{"cell_type":"markdown","source":"## 5.3: Split the data","metadata":{}},{"cell_type":"code","source":"# slip data\ny_train = df_Nazca_Mw['1982-08-01':'2014-11-01']\ny_test = df_Nazca_Mw['2014-11-01':'2016-12-01']\n\n# percentage training\nPercentage_training = (len(y_train)*100)/(len(y_train)+len(y_test))\nPercentage_testing = (len(y_test)*100)/(len(y_train)+len(y_test))\nprint('The data is splited in {:.2f}% training and {:.2f}% testing.'.format(Percentage_training,Percentage_testing))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\n\nplt.title('Time Series Split',size=16) \n\nplt.plot(y_train['log_Energy'],'b-',label = 'training')\nplt.plot(y_test['log_Energy'],'r-',label = 'testing')\nplt.ylabel('monthly seismic energy released (log)', fontsize=15)\nplt.tick_params(axis='both', which='major', labelsize=14)\nplt.tick_params('y', colors='darkblue')\nplt.legend(loc='best')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.4: Forecasting a Time Series with ARIMA\nARIMA stands for Auto-Regressive Integrated Moving Averages. The ARIMA forecasting for a stationary time series is nothing but a linear (like a linear regression) equation. The predictors depend on the parameters (p,d,q) of the ARIMA model:\n\n- 1: **Number of AR (Auto-Regressive) terms (p)**: AR terms are just lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)….x(t-5).\n\n- 2: **Number of MA (Moving Average) terms (q)**: MA terms are lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)….e(t-5) where e(i) is the difference between the moving average at ith instant and actual value.\n\n- 3: **Number of Differences (d)**: These are the number of nonseasonal differences, i.e. in this case we took the first order difference. So either we can pass that variable and put d=0 or pass the original variable and put d=1. Both will generate same results.\n\nTo determine the value of ‘p’ and ‘q’, we use two plots ACF and PACF:\n\n- **Autocorrelation Function (ACF)**: It is a measure of the correlation between the TS with a lagged version of itself. For instance at lag 5, ACF would compare series at time instant ‘t1’…’t2’ with series at instant ‘t1-5’…’t2-5’ (t1-5 and t2 being end points).\n- **Partial Autocorrelation Function (PACF)**: This measures the correlation between the TS with a lagged version of itself but after eliminating the variations already explained by the intervening comparisons. Eg at lag 5, it will check the correlation but remove the effects already explained by lags 1 to 4.","metadata":{}},{"cell_type":"code","source":"adf = adfuller(y_test['log_Energy'])\nprint(\"p-value of microsoft: {}\".format(float(adf[1])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\nimport statsmodels.api as sm\nfig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(y_train['log_Energy'],lags=40,ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(y_train['log_Energy'],lags=40,ax=ax2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This time series has: Constant mean, constant variance and a Zero auto-correlation at all lags, it is white noise. It cannot be predict.","metadata":{}},{"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\nimport statsmodels.api as sm\nfig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(y_train['count'],lags=20,ax=ax1,color='green')\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(y_train['count'],lags=20,ax=ax2,color='green')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this plot, the two dotted lines on either sides of 0 are the confidence interevals. These can be used to determine the ‘p’ and ‘q’ values as:\n\np – The lag value where the PACF chart crosses the upper confidence interval for the first time. Here, p=1.\n\nq – The lag value where the ACF chart crosses the upper confidence interval for the first time. here, q=1","metadata":{}},{"cell_type":"markdown","source":"### 5.4: ARIMA model for number eartquake","metadata":{}},{"cell_type":"code","source":"model = ARIMA(y_train['count'], order=(1,0,1))\nresults_ARIMA = model.fit()\n\nprint(results_ARIMA .summary())\n\nfig = plt.figure(figsize=(12,8))\nplt.plot(y_train['count'])\nplt.plot(results_ARIMA.fittedvalues, color='green')\nplt.title('RSS: %.4f'%sum((results_ARIMA.fittedvalues - y_train['count'])**2))\nprint('Plotting ARIMA model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot residual errors\nresiduals = pd.DataFrame(results_ARIMA.resid)\nfig, ax = plt.subplots(1,2,figsize=(15,6))\nresiduals.plot(title=\"Residuals on count\", ax=ax[0])\nresiduals.plot(kind='kde', title='Density', ax=ax[1])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fcast = results_ARIMA.get_forecast(steps=122).summary_frame()\nfcast = results_ARIMA.get_forecast(steps=122).summary_frame()\nplt.figure(figsize=(10,5))\n\nplt.title('Forecast',size=16) \n\nplt.plot(y_train['count'],'b-',label = 'training')\nplt.plot(y_test['count'],'r-',label = 'testing')\nplt.plot(fcast['mean'],'g',label = 'prediction')\nplt.fill_between(fcast.index, fcast['mean_ci_lower'], fcast['mean_ci_upper'], color='k', alpha=0.1);\nplt.ylabel('monthly seismic energy released (log)', fontsize=15)\nplt.tick_params(axis='both', which='major', labelsize=14)\nplt.tick_params('y', colors='darkblue')\nplt.legend(loc='best')\nplt.show()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the data doesn't have strong seasonality and the model finds difficult to predict the future therefore it simply take average of your previous values and predict as future. There fore we are getting straight line.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}